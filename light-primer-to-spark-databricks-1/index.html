<!doctype html><html><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1"><title>A Light Primer to Spark / Databricks</title><meta name=description content="A partnership between international organizations and companies, created to facilitate the use of third-party data in research and international development."><link rel=stylesheet href=https://stackpath.bootstrapcdn.com/bootstrap/4.3.1/css/bootstrap.min.css integrity=sha384-ggOyR0iXCbMQv3Xipma34MD+dH/1fQ784/j6cY/iJTQUOhcWr7x9JvoRxT2MZw1T crossorigin=anonymous><link rel=stylesheet href=//datapartnership.org/css/main.css><link href="https://fonts.googleapis.com/css?family=Abel&display=swap" rel=stylesheet><link href="https://fonts.googleapis.com/css?family=Roboto:400,900&display=swap" rel=stylesheet><link rel="shortcut icon" href=/favicon.ico type=image/x-icon><link rel=icon href=/favicon.ico type=image/x-icon><meta property="og:type" content="website"><meta property="og:title" content="A Light Primer to Spark / Databricks"><meta property="og:description" content="A partnership between international organizations and companies, created to facilitate the use of third-party data in research and international development."><meta property="og:image" content="/dp-facebook-card.jpg"><meta name=twitter:card content="summary"><meta name=twitter:title content="A Light Primer to Spark / Databricks"><meta name=twitter:description content="A partnership between international organizations and companies, created to facilitate the use of third-party data in research and international development."><meta name=twitter:image content="/dp-twitter-card.jpg"><meta name=referrer content="no-referrer"><script async src="https://www.googletagmanager.com/gtag/js?id=UA-132319281-1"></script><script type=text/javascript src=//datapartnership.org/js/ga.js></script><meta name=google-site-verification content="JobK4G6UDyfYh8ELvb0lfeHQK3_N3TJFKWAM5SWudd8"><meta name=keywords content="datapartnership.com,international consortium of data collaborative,international consortium of data collaboratives world bank,data consortium,data international development,data international public good,data public good,data global social impact,data social good,data partnership world bank google,public private data partnership"><body><div id=content><div id=content><section><div class="container-fluid bg-primary-gradient text-light" style=height:220px><div class="container pb-5"><div class="row pt-3"><div class="col-12 h6"><a href=/><img class=img-fluid src=/logo-title.png width=40% height=40% alt="A Light Primer to Spark / Databricks"></a></div></div></div></div></section><section><div class="container-fluid bg-secondary"><div class=container><div class=row><div class=col-lg-23><nav aria-label="breadcrumb text-light"><ol class="breadcrumb bg-secondary m-0"><li class=breadcrumb-item><a class=text-light href=//datapartnership.org/ aria-current=page>Home</a></li><li class=breadcrumb-item><a class=text-light href=//datapartnership.org/updates/ aria-current=page>Updates</a></li><li class=breadcrumb-item><a class=text-light href=//datapartnership.org/light-primer-to-spark-databricks-1/ aria-current=page>A Light Primer to Spark / Databricks</a></li></nav></div></div></div></div></section><section><div class="container mt-5 mb-5"><div class=row><div class="col-md-8 col-xs-12 single-content blog-content"><h1>: A Light Primer to Spark / Databricks</h1><hr><p><a href=https://databricks.com/>Databricks</a> is a powerful platform for using Spark, a powerful data technology.</p><p>If you&rsquo;re reading this, you&rsquo;re likely a Python or R developer who begins their Spark journey to process large datasets.</p><p>In this post, I try to provide a very general overview of the things that confused me when using these tools. I hope to help you avoid costly mistakes, saving you some time and some money.</p><p>If I missed something or got something wrong, I would love a <a href=https://twitter.com/mrmaksimize>tweet from you</a> letting me know. I would much prefer that over you cursing my name under your breath for the next several months.</p><p><img src=https://i.imgur.com/fOfA1je.png alt></p><h2 id=tldr>TLDR</h2><ul><li>Spark is a framework that enables large data to be processed by parallelizing workloads across multiple machines, or clusters.</li><li>Databricks is a service that makes it easy to work with Spark (like GitHub for Git)</li><li>There are two types of commands in Spark: <strong>Transformations</strong> and <strong>Actions</strong>; <strong>Transformations</strong> do not run on the entire dataset, but are queued up and executed when you run an <strong>action</strong>.</li><li>If you use Pandas in Spark, you lose all the power of Spark parallelization.</li><li>Use Spark Dataframes, not Pandas dataframes or RDDs.</li><li>The name of the game in Spark is to:<ul><li><strong>Keep data and workloads spread evenly across nodes.</strong></li><li><strong>Ensure that data size distributed to any given node does not exceed that node&rsquo;s RAM capacity.</strong></li></ul></li><li>Databricks runs on top of Azure or AWS. That means you pay for Databricks AND for Azure or AWS compute. Here&rsquo;s how to think about cost:<ul><li><strong>How long are my nodes going to be turned on.</strong></li><li><strong>How long are my nodes going to be &ldquo;crunching data&rdquo;.</strong></li><li><strong>How much storage am I going to use, and what are the costs for reading and writing from that storage.</strong></li></ul></li></ul><h2 id=spark-vs-databricks>Spark vs. Databricks</h2><p>Let&rsquo;s clear up the difference between Spark and Databricks. Spark is a distributed computing framework for working with large datasets. Databricks hosts that technology, making it easier to use (they also contribute heavily to the Spark open-source project).</p><p>Think of Git and GitHub: you can use Git with or without GitHub, but GitHub makes a few things a lot easier.</p><h2 id=spark-overview>Spark Overview</h2><p>A cluster is a configuration of machines (or nodes) that work together to accomplish a parallelization task.</p><p>Spark is a distributed computing platform. When you run a job in Spark, the driver node in your cluster decides the best way to distribute data across the worker nodes based on the operation and the data you are operating on. Each node (the driver node and the worker nodes) are separate machines (or virtual machines) with a specific configuration (# CPU cores, RAM, and runtime version).</p><p><img src=https://i.imgur.com/IGe5bJd.png alt=Cluster></p><p>Generally, Spark is pretty good at deciding how to spread work across nodes, but you have the option to assume more control if you need to. The standard abstraction level in Spark is a Spark Dataframe. If you use a SparkDF, you&rsquo;re letting Spark make most decisions about how the job executes.</p><p>If you run into performance issues, you will likely want to see if there is data skew (when variable amounts of work are getting assigned to different nodes) and potentially repartition.</p><p>As a last resort, you may need to drop to a less abstract version of a SparkDF, a SparkRDD (Resilient Distributed Dataset - drop that at a dinner party!). With an RDD, you can be much explicit about how your analysis works.</p><p>To keep this brief, I am explicitly skipping partitioning and RDDs. But I may write about these later.</p><p>If you remember nothing else from this article so far, remember this: the name of the game in Spark is to:</p><ul><li><strong>Keep data and workloads spread evenly across nodes.</strong></li><li><strong>Ensure that data size distributed to any given node does not exceed that node&rsquo;s RAM capacity.</strong></li></ul><h2 id=interacting-with-spark>Interacting with Spark</h2><p>In Python or R, you write some code in a notebook cell, you run it, and the data gets assigned to a variable.
In Spark, there are two types of commands: <strong>transformations</strong> and <strong>actions</strong>.</p><p><img src=https://i.imgur.com/qvkTq9h.png alt="Transformations v Actions">
<em><a href=https://training.databricks.com/visualapi.pdf>Source: Databricks Visual API</a></em></p><p><strong>Transformations</strong> change the data in some way, like a <code>filter</code>, <code>sort</code>, or <code>groupBy</code>. When Spark evaluates <strong>Transformations</strong>, it won&rsquo;t execute the computation.</p><p>When you run an <strong>Action</strong> command, Spark will evaluate all the queued up <strong>transformations</strong> before the <strong>action</strong> call. Then it will generate a DAG (Directed Acyclic Graph) that has the most efficient computation path. Finally, Spark will execute the DAG. <strong>Actions</strong> commands include <code>sum</code>, <code>display</code>, <code>top</code>, and many others.</p><p>For example, say you want to perform a <code>filter</code>, a <code>groupBy</code>, then calculate a <code>sum</code> for each group. In Python, each command would run independently, taking time, and using compute resources. In Spark, all the commands would run once, in the most efficient way possible.</p><p>For great visual representations of available transformations and actions, check out the <a href=https://training.databricks.com/visualapi.pdf>Visual API</a></p><h2 id=the-spark-api>The Spark API</h2><p><img src=https://2s7gjr373w3x22jf92z99mgm5w-wpengine.netdna-ssl.com/wp-content/uploads/2019/03/spark_graphic.png alt="Spark Layout">
Scala, Python, and R have libraries for interacting with the Spark Engine. It&rsquo;s surprisingly easy to switch languages since the API wrappers for each language are consistent.</p><p>In Databricks, you can set the language at the notebook level:</p><p><img src=https://i.imgur.com/CK2jRs6.png alt="Set language at Notebook Level"></p><p>or use flags within the cell to enable language evaluation just for that cell:</p><p><img src=https://i.imgur.com/0qg2xeY.png alt="Language flag"></p><p>Just remember that to get Spark&rsquo;s full power, you need to <strong>use a Spark DataFrame</strong>, not the dataframe of the language you&rsquo;re using (<code>pandas.DataFrame</code> in Python or Data Frame in R).</p><h3 id=can-i-use-pandas>Can I Use Pandas?</h3><p>In Databricks, this is legitimate to do:</p><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=color:#f92672>import</span> pandas <span style=color:#f92672>as</span> pd
df <span style=color:#f92672>=</span> pd<span style=color:#f92672>.</span>read_csv(<span style=color:#e6db74>&#34;/path/to/file&#34;</span>)
</code></pre></div><p>This will read the file into a <code>pandas.Dataframe</code>.
This will <strong>not</strong> get you a <strong>Spark Dataframe</strong>.</p><p><img src=https://3qeqpr26caki16dnhd19sv6by6v-wpengine.netdna-ssl.com/wp-content/uploads/2014/06/pandas-for-data-analysis.jpg alt=Pandas></p><p><em>Sad panda.</em></p><p>Spark does not parallelize Pandas dataframes. It only parallelizes Spark Dataframes. That means your Pandas dataframe will only run on a single node of your cluster - the driver. The rest of the nodes will sit there, twiddling their thumbs. So if you&rsquo;re using Pandas, no matter how many nodes you have, you will always be constrained by the driver node&rsquo;s memory / CPU.</p><p>To make full use of Spark, create a Spark Dataframe:</p><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python>df <span style=color:#f92672>=</span> spark<span style=color:#f92672>.</span>read<span style=color:#f92672>.</span>format(<span style=color:#e6db74>&#39;csv&#39;</span>)<span style=color:#f92672>.</span>load(<span style=color:#e6db74>&#34;/path/to/file&#34;</span>)
</code></pre></div><p>Spark will know how to distribute it across the nodes of the cluster. However, what you gain in performance, you lose in syntactic sugar that Pandas provides.</p><p>You will likely stumble into this command as you&rsquo;re Googling around:</p><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python>pandasDf <span style=color:#f92672>=</span> sparkDf<span style=color:#f92672>.</span>toPandas()
</code></pre></div><p>This command does indeed convert a Spark Dataframe into Pandas one. However, expect this to take a while for a large dataset.
The driver will have to pull all the data partitions back from the cluster nodes back to the single driver node. If that data size exceeds the driver&rsquo;s RAM capacity, the command will crash (likely after several hours).</p><p>You can still have a Panda-like interface if you use <a href=https://koalas.readthedocs.io/en/latest/getting_started/10min.html>the Koalas library</a>. However, I can&rsquo;t vouch for it since I haven&rsquo;t used it yet.</p><h3 id=what-about-sql>What about SQL?</h3><p>In Spark, you can interchangeably use the Spark API and SQL to perform transformations and actions on Spark Dataframes.</p><p>For example,</p><p>this:</p><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python>result <span style=color:#f92672>=</span> df<span style=color:#f92672>.</span>filter(df<span style=color:#f92672>.</span>state <span style=color:#f92672>==</span> <span style=color:#e6db74>&#34;IL&#34;</span>)<span style=color:#f92672>.</span>show()
</code></pre></div><p>this:</p><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python>df<span style=color:#f92672>.</span>createOrReplaceTempView(<span style=color:#e6db74>&#34;myDF&#34;</span>)

result <span style=color:#f92672>=</span> spark<span style=color:#f92672>.</span>sql(<span style=color:#e6db74>&#34;SELECT * FROM myDF WHERE myDF.state = &#34;</span>IL<span style=color:#e6db74>&#34;)</span>
</code></pre></div><p>and finally this:</p><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=color:#f92672>%</span>python
df<span style=color:#f92672>.</span>createOrReplaceTempView(<span style=color:#e6db74>&#34;myDF&#34;</span>)
</code></pre></div><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-sql data-lang=sql><span style=color:#f92672>%</span><span style=color:#66d9ef>sql</span>
<span style=color:#66d9ef>SELECT</span> <span style=color:#f92672>*</span> <span style=color:#66d9ef>FROM</span> myDF <span style=color:#66d9ef>WHERE</span> myDF.<span style=color:#66d9ef>state</span> <span style=color:#f92672>=</span> <span style=color:#e6db74>&#34;IL&#34;</span>
</code></pre></div><p>Are effectively the same. The single exception is, you cannot assign the last example to a variable.</p><p>As you get into using Spark / Databricks, you will likely find that each approach is useful in its way.</p><h2 id=rdds-and-udfs>RDDs and UDFs</h2><p>As you&rsquo;re reading Spark or Databricks documentation, you will run into frequent mentions of RDDs and UDFs. Let&rsquo;s have a quick look at what they are, so you can speak the language.</p><h4 id=rdd>RDD</h4><p>An RDD, or Resilient Distributed Dataset, is a core abstraction level in Spark. It is a fault-tolerant collection of elements that Spark can operate on in parallel across clusters. In the beginning, you will likely not work with RDDs, because Spark Dataframes provide a higher-level abstraction that makes it easier to work with data in Spark.</p><h4 id=udf>UDF</h4><p>UDFs are User Defined Functions in Spark. In Python, when you create a function and pass it to <code>df.apply()</code> - it&rsquo;s a similar idea. UDFs make things convenient, but Spark does not know how to optimize them. In the beginning, it&rsquo;s best not to write your own.</p><h2 id=installing-libraries>Installing Libraries</h2><p>The experience of installing libraries brings us to the first significant divergence of Spark and Databricks. If you are running Spark in a Docker container, installing libraries is just a regular <code>pip install</code>.</p><p>Databricks, on the other hand, has many <a href=https://docs.databricks.com/libraries/index.html>libraries preinstalled already</a>. Before installing something, it&rsquo;s a good idea to try to <code>import</code> it and see if you get an error. If you do, head over to <strong>Clusters > Libraries</strong>, and install what you need. Just make sure your cluster is on.</p><p><img src=https://i.imgur.com/xHSkXIQ.png alt></p><h2 id=storage>Storage</h2><p>Storage was another thing in Databricks that took a bit of time to understand.</p><h3 id=dbfs>DBFS</h3><p>The Databricks File System or DBFS provides a way to interact with files stored in Databricks. The file system itself, however, is an abstraction. DBFS encompasses files you manually uploaded to Databricks (usually stored at <code>/</code>, <code>/user</code>, <code>/FileStore</code>), mountpoints (<code>/mnt</code>), as well as other things.</p><p>DBFS makes things very convenient. You can mount an S3 Bucket at <code>/mnt/S3_BucketName</code>, and an Azure Data Lake at <code>/mnt/ADLS_NAME</code>, and mix data from these two sources seamlessly in your analysis.</p><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=color:#75715e># Read Data</span>
df <span style=color:#f92672>=</span> spark<span style=color:#f92672>.</span>read<span style=color:#f92672>.</span>format(<span style=color:#e6db74>&#34;csv&#34;</span>)<span style=color:#f92672>.</span>load(<span style=color:#e6db74>&#34;dbfs:/mnt/S3_BucketName/file.csv&#34;</span>)

<span style=color:#75715e># Do some stuff</span>
<span style=color:#f92672>...</span>

<span style=color:#75715e># Write Data</span>
spark<span style=color:#f92672>.</span>write<span style=color:#f92672>.</span>format(<span style=color:#e6db74>&#34;delta&#34;</span>)<span style=color:#f92672>.</span>save(<span style=color:#e6db74>&#34;dbfs:/mnt/ADLS_NAME/output_delta_lake&#34;</span>)
</code></pre></div><p>Keep in mind that anything you store outside of <code>/mnt/YOUR_MOUNT_X</code> will live on Databricks instances. After a quick Google search, I couldn&rsquo;t figure out how much it costs. But suffice it to say, I ran up a bit of a bill not knowing that, so I suggest avoiding it.</p><h3 id=hive-metastore>Hive Metastore</h3><p><img src=https://i.imgur.com/neA1Y1D.png alt>
&ldquo;Hive Metastore&rdquo; is about as much of a cool buzzword as it gets, and it lives in the &ldquo;Data&rdquo; tab in Databricks. This is a nice <em>relational-if-you-want-it</em> database that Databricks maintains to make life easier and use SQL.</p><p>To put a Spark Dataframe into the Metastore:</p><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python>df<span style=color:#f92672>.</span>createOrReplaceTempView(tableName)
</code></pre></div><p>The Metastore has several layers of &ldquo;persistence&rdquo;. You can create temporary tables just for the session duration, tables available to all users, or tables available to only one user.</p><p>This could be another blog post, so we&rsquo;ll leave it here for such a thing.</p><h3 id=delta-lake>Delta Lake</h3><p>The Hive Metastore doesn&rsquo;t have much of a relationship to DBFS, except in a Delta Lake. Delta Lake, created by Databricks, is a data format heavily based on Parquet. Mounting Delta Lake files from DBFS to the Hive Metastore will make Databricks automatically keep the two in sync. So when you change data in the Hive Metastore or write new data to Delta files, its counterpart will update accordingly.</p><p>Delta is also versioned, keeping a granular record of every data change. This lets you write SQL or Spark to &ldquo;time travel&rdquo; across your data, using a version or timestamp. This has helped me recover from a dumb mistake many times.</p><p>The <a href=https://docs.databricks.com/delta/delta-batch.html>Databricks' documentation</a> is pretty good here, so I&rsquo;ll let you read it if interested.</p><h2 id=pricing>Pricing</h2><p><img src=https://i.imgur.com/7It4KqH.png alt></p><p>Databricks pricing is complicated.</p><p>If you look at the Databricks Pricing page for <a href=https://databricks.com/product/azure-pricing>Azure</a> or <a href=https://databricks.com/product/aws-pricing>AWS</a>, you pay a certain amount of cents per Databricks Unit (DBU).</p><p>Except&mldr; what&rsquo;s a DBU? And why are we looking at Azure and AWS? Aren&rsquo;t we using Databricks?</p><p>Databricks' core value offering is to provide managed Spark and interactive notebooks on top of cloud infrastructure. Databricks does NOT offer the cloud infrastructure itself. In this instance, by cloud infrastructure, I explicitly mean <em>storage</em> and <em>compute</em>.</p><p>That job is outsourced to AWS or Azure - as it should - but they still need to make money, so they charge you for cloud infrastructure as well. Again&mldr;storage and compute.</p><p>Any given Databricks analysis cost is going to consist of:</p><ul><li><strong>How long are my nodes going to be turned on:</strong> <em>(This is the price you pay for AWS for keeping your nodes running)</em></li><li><strong>How long are my nodes going to be &ldquo;crunching data&rdquo;:</strong> <em>(This is the price you pay to Databricks (DBUs) that get counted when a computation is running.)</em></li><li><strong>How much storage am I going to use, and what are the costs for reading and writing from that storage:</strong> <em>(This is the price you pay to AWS for using S3. Ideally, if you&rsquo;re using EC2 instances and you&rsquo;re in the same region, this should be 0)</em></li></ul><p>Knowing this, the most reasonable way to optimize costs is to work on a tiny subset of your data, using a small cluster with the least nodes while you&rsquo;re developing your code. Then, scale up to more / larger nodes as you begin processing your full dataset.</p><p>This is a good rule of thumb, and cost/performance optimization here can get pretty tricky (maybe tricky enough for another blog post), so I will leave it here for now.</p><h2 id=wrapping-up>Wrapping up</h2><p>Hopefully this intro overview has been helpful. Databricks and Spark is a cool technology and we&rsquo;re excited to see what you do with it!
If I missed something or got something wrong, I would love a <a href=https://twitter.com/mrmaksimize>tweet from you</a> letting me know.</p><p>If you liked this article, please share it and tag <a href=https://twitter.com/devdatapship>@DevDataPship</a> on Twitter!</p><p><em>Author(s):</em></p><ul><li><a href=https://maksimpecherskiy.com>Maksim Pecherskiy</a>, <em>Data Engineer, Development Data Partnership, World Bank</em></li></ul><p><em>The Development Data Partnership is a partnership between international organizations and companies, created to facilitate the use of third-party data in research and international development.</em></p></div><div class="col-md-3 offset-md-1 col-xs-12"><div class="bg-secondary text-light p-3"><h4 class=mb-4>You can also read</h4><hr><dl><dt class=pb-4><a class="h5 text-light" href=/covid19-lockdown-measures-internet-speed-africa/>The Effect of COVID-19 Lockdown Measures on Internet Speed</a></dt><dt class=pb-4><a class="h5 text-light" href=/economic-impact-covid-19-in-india-through-daily-electricity-consumption-and-nighttime-light-intensity/>Examining the Economic Impact of COVID-19 in India through Daily Electricity Consumption and Nighttime Light Intensity</a></dt><dt class=pb-4><a class="h5 text-light" href=/updates/optimizing-tourism-investments-identifying-chokepoints-and-underperforming-minor-roads-in-sri-lanka/>Optimizing Tourism Investments: Identifying Chokepoints and Underperforming Minor Roads in Sri Lanka</a></dt><dt class=pb-4><a class="h5 text-light" href=/india-zone-classification-impact-of-covid19/>Examining the Economic Impact of Indiaâ€™s Zone Classification to contain COVID-19</a></dt><dt class=pb-4><a class="h5 text-light" href=/covid-observatory-indonesia/>Socioeconomic Impact of COVID-19 in Indonesia</a></dt><dt class=pb-4><a class="h5 text-light" href=/light-primer-to-spark-databricks-1/>A Light Primer to Spark / Databricks</a></dt><dt class=pb-4><a class="h5 text-light" href=/broadband-coverage-in-ukraine/>Estimating High-speed Broadband Coverage in Ukraine</a></dt><dt class=pb-4><a class="h5 text-light" href=/2020-inspire-challenge-finalist-locust-project/>2020 Inspire Challenge Finalist - The ClimaCell Locust Project</a></dt><dt class=pb-4><a class="h5 text-light" href=/economic-anxiety-covid19-se-asia/>Economic Anxiety and COVID-19 in Southeast Asia</a></dt><dt class=pb-4><a class="h5 text-light" href=/planning-for-equitable-access-to-health-infrastructure/>Analyzing Spatial Accessibility of Health Facilities in Indonesia and Philippines</a></dt><dt class=pb-4><a class="h5 text-light" href=/introducing-mapbox-movement-data/>Mapbox Releases a New Movement Dataset for Mobility Insights</a></dt><dt class=pb-4><a class="h5 text-light" href=/annualmeetings2020/>Data Partnership: Bridging the Data Gap for Public Good</a></dt></dl></div></div></div></div></section></div></div><footer><div class="container-fluid pb-4"><div class=row><div class=col-12><a class="footer-image p-4" href=https://www.iadb.org/en><img class=img-fluid src=//datapartnership.org/idb.svg alt=IDB></a>
<a class="footer-image p-4" href=https://www.imf.org><img class=img-fluid src=//datapartnership.org/imf.svg alt=IMF></a>
<a class="footer-image p-4" href=https://www.worldbank.org><img class=img-fluid src=//datapartnership.org/wbg.svg></a>
<a href=https://www.worldbank.org/en/about/legal>Legal</a></div></div></div></footer></body></html>